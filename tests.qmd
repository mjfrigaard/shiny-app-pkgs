# Tests {.unnumbered}

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y",
  header = "Caution!",
  contents = "This section is currently being revised. Thank you for your patience."
)
```

Writing tests for your app-package poses some unique challenges. Shiny functions are written in the context of its [reactive model](https://shiny.posit.co/r/articles/build/reactivity-overview/), so some conventional testing techniques and methods for regular R packages donâ€™t directly apply. Fortunately, the infrastructure for storing and running tests in your app-package is identical to a standard R package.

This chapter will cover three layers of tests: unit tests, integration/module tests, and end-to-end or system tests. I'll focus on *what to test* and *why to test it*, not *how to write tests*, because plenty of those resources exist.[^unit-tests-1] [^system-tests-2] The only exception being some tricks I've learned for using `testServer()` with module server functions.[^test-server-tests-3]

Iâ€™ll also touch on the links between user requirements, functional specifications, and using a traceability matrix to track issues & features.

[^unit-tests-1]: Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html)

[^system-tests-2]: `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.

[^test-server-tests-3]: The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing module server functions.

The code chunk below will load the necessary testing packages.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

## Running [`test()`]{style="font-size: 1.05em;"}s

```{r}
#| label: co_box_tests_pkgApp
#| echo: false
#| eval: false
```

 
The fourth `devtools` [habit]{style="font-weight: bold; font-size: 1.0em; color: #772953"} to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut:

::: {.column-margin}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 1.20em"}

:::


::: {#fig-08_tests_build_pane_test}

![Test you app-package](img/08_tests_build_pane_test.png){#fig-08_tests_build_pane_test width='100%' align='center'}

`devtools::test()` (run all tests in your `tests/` folder)
:::

When we initially run `devtools::test()` in `pkgApp`, we see the following:

```{verbatim}
#| eval: false
#| code-fold: false
==> devtools::test()

â„¹ No testing infrastructure found.
â€¢ Setup testing with `usethis::use_testthat()`.
```

This shouldn't be surprising--we haven't written any tests for `pkgApp` yet!

```{r}
#| label: co_box_monthApp_tests
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE,
  header = "Mastering Shiny `monthApp` tests",
" 
If you downloaded, loaded and installed [`monthApp` example from Mastering Shiny](https://github.com/hadley/monthApp), then clicked on **Test** in the **Build** pane, you also saw the following:

![Testing `monthApp` app-package](img/monthApp_test.png){width='100%' fig-align='center'}

"
  )
```

The error is informative because it tells us that `pkgApp` doesnâ€™t have the testing infrastructure set up. In packages using `devtools`, the unit testing infrastructure is built with `usethis::use_testthat()`

## The test suite

Multiple strategies exist for testing code. For example, if you've adopted test-driven development (TDD)[^test-driven-dev-4], you'll develop tests before writing utility functions, modules, or your standalone app function. However, if you're a mere mortal like the rest of us, you'll typically develop your tests and functions in tandem. 

[^test-driven-dev-4]: "The tests should be written before the functionality that is to be tested. This has been claimed to have many benefits. It helps ensure that the application is written for testability, as the developers must consider how to test the application from the outset rather than adding it later." - [TDD, Wikipedia](https://en.wikipedia.org/wiki/Test-driven_development) 

Regardless of the testing strategy, we should follow the advice presented to us in the output above and set up the testing infrastructure with the [`testthat` package](https://testthat.r-lib.org/):

### [`use_testthat()`]{style="font-size: 1.05em;"}

The 'infrastructure' created by running `usethis::use_testthat()` is detailed below: 

```{r}
#| eval: false
#| code-fold: false
usethis::use_testthat()
```

-   Set active project to current working directory: 

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    âœ” Setting active project to '/path/to/pkgApp'
    ```

-   In the `DESCRIPTION` file, add the `Suggests` field and include `testthat (>= 3.0.0)` and the testthat edition (`Config/testthat/edition: 3`)

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    âœ” Adding 'testthat' to Suggests field in DESCRIPTION
    âœ” Adding '3' to Config/testthat/edition
    ```
    
-   A new `tests/` folder is created, with a `testthat/` subfolder:

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    âœ” Creating 'tests/testthat/'
    ```
    
-   The `testthat.R` file is created (sometimes referred to as the test 'runner' because it runs all your tests).
    
    ```{verbatim}
    #| eval: false
    #| code-fold: false
    âœ” Writing 'tests/testthat.R'
    ```

Finally, we're given some advice on the next step for creating our first test: 

```{verbatim}
#| eval: false
#| code-fold: false
â€¢ Call `use_test()` to initialize a basic test file and open it for editing.
```

Our new `tests/` folder structure is below: 

```{bash}
#| eval: false
#| code-fold: false
tests/
  â”œâ”€â”€ testthat
  â””â”€â”€ testthat.R

2 directories, 1 file
```


## Unit tests

```{r}
#| label: git_box_08_testthat-tests
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "08_testthat-tests", 
  repo = 'pkgApp')
```

<!--
> Test Core Functions: Use the `testthat` package to write unit tests for all core logic functions. These are the building blocks of your application and should be thoroughly tested.

> Mock Inputs: For functions that rely on external data or user input, use mock data to simulate various scenarios.

> Boundary Conditions: Test edge cases and boundary conditions to ensure stability.

> Test Coverage: Use tools like `covr` to measure test coverage and aim for a high percentage.
-->

If I'm writing write a unit test for the `scatter_plot()` function in `R/scatter_plot.R`, I'll create test file with `usethis::use_test("scatter_plot")`.

### New tests with [`use_test()`]{style="font-size: 1.05em;"}

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot")
```

#### Unit test files

The IDE will automatically open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
âœ” Writing 'tests/testthat/test-scatter_plot.R'
â€¢ Modify 'tests/testthat/test-scatter_plot.R'
```

#### Tests

The new test file contains a boilerplate test (I've included the argument names):

```{r}
#| eval: false
#| code-fold: false
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

Each `testthat` test has a test context (supplied to the `desc` argument) followed by the test `code` (supplied in curly brackets). When a test is run, you'll see feedback on whether it passes or fails:

```{r}
#| eval: true
#| code-fold: false 
#| collapse: true
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

#### Expectations

Most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against a known result (i.e., what is `expected`)

```{r}
#| eval: true 
#| code-fold: false
expect_equal(
  object = 2 * 2,
  expected = 4)
```

### Comparisons

Comparison is the backbone of testing, and I've found knowing what package is performing the underlying comparison often saves me from surprising tests results. 

For example, `testthat::expect_equal()` compares the `observed` and `expected` objects with the [`waldo` package](https://www.tidyverse.org/blog/2021/08/waldo-0-3-0/), with some help from [`diffobj`](https://github.com/brodieG/diffobj).

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true 
#| message: false 
#| warning: false
library(waldo)
library(diffobj)
library(tibble)
```

#### [`waldo`]{style="font-size: 1.05em;"}

If you'd like a preview of a comparison before writing a formal test, you can pass the your `observed` and `expected` objects to `waldo::compare()` to see what the result will be, but be mindful of the difference in argument names:

```{r}
#| eval: true 
#| include: false
old <- tibble(
  chr = LETTERS[2:4],
  num = as.double(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"), 
        levels = c("low", "med", "high"), 
        labels = c("L", "M", "H"),
        ordered = TRUE)
)
new <- data.frame(
  CHR = LETTERS[2:4],
  num = as.integer(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"),
        levels = c("low", "med", "high"),
        labels = c("low", "med", "high"))
)
```


For example, suppose we have two objects: `old` and `new`

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
old
```

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
new
```

The outputs below show us both 'No differences' and the types of differences detected with `waldo::compare()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = old)
```


```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = new)
```

:::

`compare()` displays the differences in classes, names, and any individual value differences. 

#### [`diffobj`]{style="font-size: 1.05em;"}

If you're using Posit Workbench, the [`diffobj` package](https://github.com/brodieG/diffobj) has a colorful display for making comparisons in the IDE. 

The differences can be displayed vertically with `diffobj::diffObj()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffObj(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffObj()`](img/08_tests_diffobj.png){#fig-08_tests_diffobj width='85%' align='center'}

:::

If you want to view the structure (`str()`) differences, you can use `diffobj::diffStr()`:

::: {layout="[[30, 70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffStr(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffStr()`](img/08_tests_diffstr.png){#fig-08_tests_diffobj width='100%' align='center'}
:::

After seeing the `old` vs `new` comparisons with `waldo` and `diffobj`, you should notice the similarities in the results of a `testthat` test on the same objects:

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]

â”€â”€ Failure (test-old-vs-new.R:18:3): old vs. new â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
`old` (`actual`) not equal to `new` (`expected`).

`class(actual)`:   "tbl_df" "tbl" "data.frame"
`class(expected)`:                "data.frame"

`names(actual)`:   "chr" "num" "fct"
`names(expected)`: "CHR" "num" "fct"

`actual$chr` is a character vector ('B', 'C', 'D')
`expected$chr` is absent

`class(actual$fct)`:   "ordered" "factor"
`class(expected$fct)`:           "factor"

`levels(actual$fct)`:   "L"   "M"   "H"   
`levels(expected$fct)`: "low" "med" "high"

`actual$CHR` is absent
`expected$CHR` is a character vector ('B', 'C', 'D')
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]
```


### Test supporting files

Sometimes testing functions in isolation is not enough and it's useful to include either helper functions or data. This is especially true when the code we're testing goes beyond the function's borders, such as connecting to a database or API. The term for these supporting files are **test fixtures**,[^test-fixtures-5] and below we'll cover including them in the `tests/` folder.

[^test-fixtures-5]: Test fixtures are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-advanced.html#test-fixtures). 

#### Test data 

Test data can be helpful for experimenting with various table and graphing package outputs, file formats, and application performance. Test data files can be stored in `tests/testthat/fixtures/`, and I've provided an example below of of the tidy `ggplot2movies` data we used in a previous branch.

The code used to create the test data (`make-ggp2-movies.R`) is stored in the same location as the output it creates (i.e., `ggp2_movies.rds`):

```{r}
#| eval: false 
#| code-fold: true 
#| code-summary: 'show/hide make-ggp2-movies.R'
# code to prepare `ggp2movies` test data
# pkgs <- c('ggplot2movies', 'tidyr', 'dplyr', 'stringr', 'purrr')
# install.packages(pkgs, quiet = TRUE)

# load packages --------------------
library(tidyr)
library(dplyr)
library(stringr)
library(purrr)

ggp2movies <- ggplot2movies::movies |>
  dplyr::mutate(id = 1:n()) |>
  tidyr::pivot_longer(
    cols = Action:Short,
    names_to = "Genre",
    values_to = "GenreMember"
  ) |>
  dplyr::group_by(
    dplyr::across(-c(Genre, GenreMember))) |>
  tidyr::nest() |>
  dplyr::mutate(
    Genres = purrr::map(data, ~ if (all(.x$GenreMember == 0)) {
      character(0)
    } else {
      .x$Genre[.x$GenreMember == 1]
    })
  ) |>
  dplyr::ungroup() |>
  dplyr::select(title, Genres, length, year, 
                budget, rating, votes, mpaa) |>
  dplyr:::mutate(
    genres = purrr::map_chr(
      .x = Genres,
      .f = stringr::str_c, collapse = ", "
    )
  ) |>
  dplyr::select(title, genres, length, year, 
                budget, avg_rating = rating, 
                votes, mpaa) |>
  dplyr::mutate(
    mpaa = dplyr::na_if(x = mpaa, y = ""),
    mpaa = factor(mpaa,
      levels = c("G", "PG", "PG-13", "R", "NC-17"),
      labels = c("G", "PG", "PG-13", "R", "NC-17")
    ),
    genres = dplyr::na_if(x = genres, ""),
    genres = factor(genres)
  ) |> 
  tidyr::drop_na()
# save to tests/testthat/fixtures/
saveRDS(object = ggp2movies, file = "tests/testthat/fixtures/ggp2_movies.rds")
```

```{bash}
#| eval: false 
#| code-fold: false 
tests/
  â””â”€â”€ testthat/
      â””â”€â”€ fixtures/
            â”œâ”€â”€ ggp2_movies.rds
            â””â”€â”€ make-ggp2-movies.R
       
3 directories, 2 files
```

#### Test helpers 

Test helpers are typically functions that make testing a little easier. I've included a small example function that includes nicely formatted messages for testing, `test_cmt()`.

```{r}
#| code-fold: true
#| code-summary: 'show/hide test_cmt() helper' 
# test comment helper
test_cmt <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n", 
      stringr::str_glue("[{Sys.time()}| {msg}]"), 
      "\n")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n", 
      stringr::str_glue("[ START | {Sys.time()} | {start} = {msg}]"),
      "\n")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n", 
      stringr::str_glue("[ END   | {Sys.time()} | {end} = {msg}]"), 
      "\n")
  } else {
    cat("\n", 
      stringr::str_glue("[ START | {Sys.time()} | {start} = {msg}]"),
      "\n")
    cat("\n", 
      stringr::str_glue("[ END   | {Sys.time()} | {end} = {msg}]"),
      "\n")
  }
}
```

`test_cmt()` can be used to log when your test beings and ends, and includes a message for the context.[^logger-6]

[^logger-6]: If you like verbose logging outputs, check out the [`logger` package](https://daroczig.github.io/logger/) 

```{r}
#| collapse: true
#| code-fold: false
test_that(desc = "multiplication works", code = {
  test_cmt(start = "multiplication", msg = "2 * 2 = 4")
  expect_equal(2 * 2, 4)
  test_cmt(end = "multiplication", msg = "2 * 2 = 4")
})

```

Functions like `test_cmt()` can be stored in `tests/testthat/helper.R`, which is automatically loaded with `devtools::load_all()`:

```{verbatim}
#| eval: false 
#| code-fold: false
tests/
  â””â”€â”€ testthat/
      â”œâ”€â”€ fixtures/
      â”‚   â”œâ”€â”€ make-ggp2-movies.R
      â”‚   â””â”€â”€ ggp2_movies.rds
      â”œâ”€â”€ helper.R
      â””â”€â”€ ... all test files...
```


### Snapshots

Writing tests for graph outputs can be difficult because we're evaluating the "correctness" of the graph is somewhat subjective and requires human judgment.

If we try to compare the output from a custom plotting function like `scatter_plot()` against a graph built with analogous `ggplot2` code, we can see why this test will fail by passing both objects to `diffobj::diffObj()`:

```{r}
#| eval: false 
#| code-fold: false
ggp_graph <- ggplot2::ggplot(mtcars, 
              ggplot2::aes(x = mpg, y = disp)) + 
              ggplot2::geom_point(
                ggplot2::aes(color = cyl), 
                             alpha = 0.5, 
                             size = 3)
  
app_graph <- scatter_plot(mtcars, 
                  x_var = "mpg", 
                  y_var = "disp", 
                  col_var = "cyl", 
                  alpha_var = 0.5, 
                  size_var = 3)

diffobj::diffObj(ggp_graph, app_graph)
```


:::: {.column-page-inset-right}

:::{#fig-08_tests_diffobj_scatter_plot}

![`diffobj::diffObj()` on graph outputs](img/08_tests_diffobj_scatter_plot.png){#fig-08_tests_diffobj_scatter_plot width='100%' align='center'}

Graph objects are difficult to use as test objects 
:::

::::

The output shows us differences in the `mapping` and plot environment (`plot_env`), which we can assume *will* be different in the application. 

In cases like this, a snapshot test might be warrented. The [`vdiffr`](https://vdiffr.r-lib.org/) package allows us to perform a 'visual unit test' by saving by capturing the expected output as a snapshot that we can compare with future versions.

#### [`vdiffr`]{style="font-size: 1.05em;"}

The `expect_doppelganger()` function from `vdiffr` is designed specifically to work with ['graphical plots'](https://vdiffr.r-lib.org/reference/expect_doppelganger.html). 

```{r}
#| eval: false 
#| code-fold: false
vdiffr::expect_doppelganger(
      title = "name of graph", 
      fig = # ...code to create graph...
  )
```


`expect_doppelganger()` can be dropped in `test_that()` like any other expectation. Below is an example comparing the graph outputs from `ggplot2` and the custom `scatter_plot()` utility function in `pkgApp`. 

Notice I've loaded the `ggp2_movies.rds` data[^test-path-7] and included the `test_cmt()` helper with `start` and `end` messages (before and after the test code).

[^test-path-7]: Accessing test files is made easier with `testtthat::test_path()`

```{r}
#| eval: false 
#| code-fold: false
test_that(desc = "scatter_plot() works", code = {
  test_cmt(
    start =  "scatter_plot()", 
    msg = "ggplot2movies::movies snapshot")
  ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
    vdiffr::expect_doppelganger(
      title = "scatter_plot() graph", 
      fig = scatter_plot(ggp2_movies, 
                    x_var = "budget", 
                    y_var = "avg_rating", 
                    col_var = "mpaa", 
                    alpha_var = 0.4, 
                    size_var = 2.5))
    test_cmt(
      end =  "scatter_plot()", 
      msg = "ggplot2movies::movies snapshot")
})
```

When we initially run the test it passes, but with a warning that tells us the baseline snapshot was saved in `tests/testthat/_snaps/`:

```{verbatim}
#| eval: false 
#| code-fold: false
==> Testing R file using 'testthat'

â„¹ Loading pkgApp
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
 [ START | 2023-09-13 11:58:04 | scatter_plot() = ggplot2movies::movies snapshot] 
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]
 [ END   | 2023-09-13 11:58:05 | scatter_plot() = ggplot2movies::movies snapshot] 


â”€â”€ Warning (test-scatter_plot.R:4:5): scatter_plot() works â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-graph.svg'
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]

Test complete
```

::: {.column-margin}

`test_cmt()` is handy because it let's me track how long each test takes to run (unit test are should be fast). 

:::

After the inital test run, if we view the `tests/` folder, we can see the new `tests/testthat/_snaps` is created:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  â”œâ”€â”€ testthat/
  â”‚   â”œâ”€â”€ _snaps/
  â”‚   â”‚   â””â”€â”€ scatter_plot/
  â”‚   â”‚       â””â”€â”€ scatter-plot-graph.svg
  â”‚   â””â”€â”€ test-scatter_plot.R
  â””â”€â”€ testthat.R

```

The `scatter-plot-graph.svg` file is our baseline comparison object, which is then used in future tests. 

#### Snapshots are brittle

The term "brittle" in the context of unit tests refers to their susceptibility to changes (meaningful or not). Using snapshots can produce false negativesâ€”a test fails when comparing a new graph to the baseline imageâ€”due to inconsequential changes in the graph.

In a typical workflow, we'd write additional tests, then load, document, and build the package:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>L</kbd> / <kbd>D</kbd> / <kbd>B</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

Subsequent tests in `pkgApp` will pass without a warning:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

```{verbatim}
#| eval: false 
#| code-fold: false
==> devtools::test()

â„¹ Testing pkgApp
âœ” | F W S  OK | Context
â  |         0 | scatter_plot  
 [ START | 2023-09-13 11:58:53 | scatter_plot() = ggplot2movies::movies snapshot] 
â ‹ |         1 | scatter_plot  
 [ END   | 2023-09-13 11:58:54 | scatter_plot() = ggplot2movies::movies snapshot] 
âœ” |         1 | scatter_plot

â•â• Results â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]

ğŸ˜¸
```

Another option for using snapshots for testing is the `expect_snapshot_file()` function [^expect-snapshot-file-8] but `expect_doppelganger()` is probably the better option for comparing graph outputs.

[^expect-snapshot-file-8]: Follow the `expect_snapshot_file()` example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)

## Module tests

Although theyâ€™re typically stored in a single file, modules consist of UI and server functions, which would classify their tests as â€˜integrationâ€™ or â€˜interactionâ€™ testing. Still, we can only pass the module server functions to `testServer()`, so these are still unit tests.

### [`testServer()`]{style="font-size: 0.95em;"}

`testServer()` is designed to test *reactive interactions*, which gives us the ability to write tests to verify the inputs, outputs, and returned values from module server functions. 

#### Testing initial values 

Shiny inputs are initiated with a `NULL` value, so it's tempting to want to test an `inputId` exists using `expect_null(input$x)`. However, this test is not helpful because *any* value will pass this test: 

```{r}
#| eval: false 
#| include: true 
#| code-fold: false
# this test passes...
shiny::testServer(app = mod_var_input_server, expr = {
  test_cmt(start = "input$x", msg = "initial null")
  testthat::expect_null(input$x)
  test_cmt(end = "input$x", msg = "initial null")
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
 [ START | 2023-09-14 11:58:59 | input$x = initial null] 
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
 [ END   | 2023-09-14 11:58:59 | input$x = initial null]  
```

```{r}
#| eval: false 
#| include: true 
#| code-fold: false
# but so does this...
shiny::testServer(app = mod_var_input_server, expr = {
  test_cmt(start = "input$anything", msg = "initial null")
  testthat::expect_null(input$anything)
  test_cmt(end = "input$anything", msg = "initial null")
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
 [ START | 2023-09-14 11:58:59 | input$anything = initial null] 
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
 [ END   | 2023-09-14 11:58:59 | input$anything = initial null] 
```

To check the initial `NULL` value of an `inputId`, it's better to build the shiny app object, then pass this to `testServer()`.[^shiny-app-obj-9]

[^shiny-app-obj-9]: The `shiny::is.shiny.appobj()` will test if an object is a 'shiny app object.'

```{r}
#| eval: false 
#| code-fold: false
# build app object with shinyApp()
app <- shinyApp(ui = movies_ui(bslib = FALSE), 
                  server = movies_server)
shiny::testServer(app = app, expr = {
  # check shiny app object 
  test_cmt(start = "is.shiny.appobj", msg = "movies_app()")
  testthat::expect_true(is.shiny.appobj(app))
  test_cmt(end = "is.shiny.appobj", msg = "movies_app()")
  # check input for y axis is initially NULL
  test_cmt(start = "input$`vars-y`", msg = "initial NULL")
  testthat::expect_null(
    object = input$`vars-y`)
  test_cmt(end = "input$`vars-y`", msg = "initial NULL")
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]

 [ START | 2023-09-14 12:08:53 | is.shiny.appobj = movies_app()] 
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
 [ END   | 2023-09-14 12:08:53 | is.shiny.appobj = movies_app()] 

 [ START | 2023-09-14 12:08:53 | input$`vars-y` = initial NULL] 
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
 [ END   | 2023-09-14 12:08:53 | input$`vars-y` = initial NULL] 
```

Note the use of the `input` with the appended module IDs. This is an easy place to get tripped up with `testServer()`: in the `app` object, we have to access the inputs with  ```` input$`[module id]-[inputId]` ````

#### Testing returned values

To confirm the returned list of graph inputs from `mod_var_input_server()`, I can use `session$setInputs()` to build the list of inputs, and `session$returned()` to confirm it's returned object:

```{r}
#| eval: false 
#| include: true
#| code-fold: false
shiny::testServer(app = mod_var_input_server, expr = {
  test_vals <- list(
                  y = "audience_score",
                  x = "imdb_rating",
                  z = "genre",
                  alpha = 0.5,
                  size = 2,
                  plot_title = "example title"
                )
  session$setInputs(
                  y = "audience_score",
                  x = "imdb_rating",
                  z = "genre",
                  alpha = 0.5,
                  size = 2,
                  plot_title = "example title"
                )
  test_cmt(start = "returned(var_input)", msg = "var_input structure")
  testthat::expect_equal(
    object = session$returned(),
    expected = test_vals
  )
  test_cmt(end = "returned(var_input)", msg = "var_input structure")
})
```

```{verbatim}
#| eval: false 
#| include: true
#| code-fold: false
==> devtools::test()

â„¹ Testing pkgApp
âœ” | F W S  OK | Context
â  |         0 | mod_var_input_server                                                  Loading required package: shiny
 [ START | 2023-09-14 12:21:17 | is.shiny.appobj = movies_app()] 
â ‹ |         1 | mod_var_input_server                          
 [ END   | 2023-09-14 12:21:17 | is.shiny.appobj = movies_app()]  

 [ START | 2023-09-14 12:21:17 | input$`vars-y` = initial NULL] 
 [ END   | 2023-09-14 12:21:17 | input$`vars-y` = initial NULL] 

 [ START | 2023-09-14 12:21:17 | returned(var_input) = var_input structure] 
 [ END   | 2023-09-14 12:21:18 | returned(var_input) = var_input structure] 
âœ” |         3 | mod_var_input_server
```


### [`args = list()`]{style="font-size: 0.95em;"}

Now that we've confirmed the returned value from `mod_var_input_server()` is a list, we want to make sure it's read correctly by the `var_inputs` argument in `mod_scatter_display_server()`. I've included the `movies_server()` function below refresh our memory of how this *should* work:[^confirm-returned-10]

```{r}
#| eval: false 
#| code-fold: false
movies_server <- function(input, output, session) {

      selected_vars <- mod_var_input_server("vars")

      mod_scatter_display_server("plot", var_inputs = selected_vars)
      
}
```

[^confirm-returned-10]: `selected_vars` are the reactive plot values returned from `mod_var_input_server()` we confirmed `test-mod_var_input_server.R`.


```{r}
#| eval: false 
#| include: false
# shiny::is.reactive(inputs()) ----------------------------------------------
shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Test title"
        )
      )
  ), expr = {
    test_cmt(start = "mod_scatter_display_server", msg = "is.reactive(inputs())")
    expect_true(
      object = is.reactive(inputs)
    )
    test_cmt(end = "mod_scatter_display_server", msg = "is.reactive(inputs())")
  }
)

# is.list(inputs()) ----------------------------------------------
shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "test title case"
        )
      )
  ), expr = {
    test_cmt(start = "mod_scatter_display_server", msg = "is.list(inputs())")
    expect_equal(
      object = inputs(),
      expected = list(
        x = "audience_score",
        y = "imdb_rating",
        z = "mpaa_rating",
        alpha = 0.5,
        size = 2,
        plot_title = "Test Title Case"
      )
    )
    test_cmt(end = "mod_scatter_display_server", msg = "is.list(inputs())")
  }
)

# ggplot2::is.ggplot2() ---------------------------------------------------

shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Test title"
        )
      )
  ), expr = {
    test_cmt(start = "mod_scatter_display_server", msg = "ggplot2::is.ggplot(plot)")
    plot <- scatter_plot(
      df = movies,
      x_var = inputs()$x,
      y_var = inputs()$y,
      col_var = inputs()$z,
      alpha_var = inputs()$alpha,
      size_var = inputs()$size
    ) +
      ggplot2::labs(
        title = inputs()$plot_title,
        x = stringr::str_replace_all(tools::toTitleCase(inputs()$x), "_", " "),
        y = stringr::str_replace_all(tools::toTitleCase(inputs()$y), "_", " ")
      ) +
      ggplot2::theme_minimal() +
      ggplot2::theme(legend.position = "bottom")
    ggplot2::is.ggplot(plot)
    test_cmt(end = "mod_scatter_display_server", msg = "ggplot2::is.ggplot(plot)")
  }
)

# print(plot) ---------------------------------------------------
# shiny::testServer(
#   app = mod_scatter_display_server,
#   args = list(
#     var_inputs =
#       shiny::reactive(
#         list(
#           x = "audience_score",
#           y = "imdb_rating",
#           z = "mpaa_rating",
#           alpha = 0.5,
#           size = 2,
#           plot_title = "Test title"
#         )
#       )
#   ), expr = {
#     test_cmt(start = "mod_scatter_display_server", msg = "print(plot)")
#     plot <- scatter_plot(
#       # data ----------------------------------------------------
#       df = movies,
#       x_var = inputs()$x,
#       y_var = inputs()$y,
#       col_var = inputs()$z,
#       alpha_var = inputs()$alpha,
#       size_var = inputs()$size
#     ) +
#       ggplot2::labs(
#         title = inputs()$plot_title,
#         x = stringr::str_replace_all(tools::toTitleCase(inputs()$x), "_", " "),
#         y = stringr::str_replace_all(tools::toTitleCase(inputs()$y), "_", " ")
#       ) +
#       ggplot2::theme_minimal() +
#       ggplot2::theme(legend.position = "bottom")
#     print(plot)
#     test_cmt(end = "mod_scatter_display_server", msg = "print(plot)")
#   }
# )
# ğŸ’… Your tests are beautiful ğŸ’…
```



<!--
> Data Flow: Test how well the different units work together by simulating a complete data flow.

> Database Interactions: If your package interacts with a database, write tests to ensure that read/write operations are working as expected.

> API Calls: If your package makes API calls, use package like httptest to mock API responses and test the integration.
-->


## Behavior-driven development (BDD)

> "*[BDD] encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.*" - [BDD, Wikipedia](https://en.wikipedia.org/wiki/Behavior-driven_development#:~:text)

### [`describe()`]{style="font-size: 0.95em;"}

The `testthat::describe()` function follows a BDD format: 

### [`it()`]{style="font-size: 0.95em;"}

`testthat::it()`

## System/end-to-end tests

<!--
> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.
-->


### [`shinytest2`]{style="font-size: 1.05em;"}

## Test coverage 

### [`covr`]{style="font-size: 1.05em;"}

### [`covrpage`]{style="font-size: 1.05em;"}

## Continuous Integration (CI)

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

end `testing.qmd`

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->